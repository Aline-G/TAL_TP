{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Sujet 1 : Evaluation de deux plateformes open source d'analyse linguistique</u>\n",
    "\n",
    "## <u> I - Evaluation de l'analyse morpho-syntaxique </u>\n",
    "\n",
    "##### 1) Utiliser le corpus annoté « pos_reference.txt.lima » pour extraire les phrases ayant servi pour produire ce corpus annoté et sauvegarder le résultat dans le fichier « pos_test.txt  »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    for line in fileI:\n",
    "\n",
    "        tuples = line.split()\n",
    "        if tuples:\n",
    "            # On souhaite récupérer l'entièreté de la ligne à l'exception du dernier mot\n",
    "            # qui est l'annotation\n",
    "            for i in range(len(tuples)-1):\n",
    "                fileO.write(tuples[i]+\" \")\n",
    "        else:\n",
    "            # Si on arrive à une ligne vide on saute une ligne c'est un changement de phrase \n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_reference.txt.lima\"\n",
    "fileOut = \"pos_test.txt\"\n",
    "\n",
    "getText(fileIn,fileOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Convertir  les  tags  du  corpus  annoté  « pos_reference.txt.lima »  en  tags  universels  et sauvegarder le résultat dans le fichier « pos_reference.txt.univ ». Il faut utiliser les deux tables de correspondance « POSTags_LIMA_PTB_Linux.txt » et « POSTags_PTB_Universal_Linux.txt » pour réaliser cette transformation (LIMA => PTB => Universal). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagUniv(fileIn, fileRef1,fileRef2, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileR1 = open(fileRef1,'r')\n",
    "    fileR2 = open(fileRef2,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    my_dico1 = {}\n",
    "    my_dico2 = {}\n",
    "\n",
    "    #on crée deux dictionnaires à partir des tags de reference\n",
    "    for line in fileR1:\n",
    "        tuples = line.split()\n",
    "        my_dico1[tuples[0]] = tuples[1]\n",
    "    fileR1.close()\n",
    "\n",
    "    for line in fileR2:\n",
    "        tuples = line.split()\n",
    "        my_dico2[tuples[0]] = tuples[1]\n",
    "    fileR2.close()\n",
    "\n",
    "    # modification des tags vers les tags univ\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        if tup:\n",
    "            # on écrit le groupe de mots initial\n",
    "            for i in range(len(tup)-1):\n",
    "                    fileO.write(tup[i]+\" \")\n",
    "            temp = my_dico1[tup[len(tup)-1]]\n",
    "\n",
    "            fileO.write('\\t'+ my_dico2[temp])\n",
    "            fileO.write('\\n')\n",
    "        else:\n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_reference.txt.lima\"\n",
    "fileRef1 = \"POSTags_LIMA_PTB_Linux.txt\"\n",
    "fileRef2 = \"POSTags_PTB_Universal_Linux.txt\"\n",
    "fileOut = \"pos_reference.txt.univ\"\n",
    "\n",
    "\n",
    "setTagUniv(fileIn,fileRef1,fileRef2,fileOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Lancer les deux POS taggers sur le fichier « pos_test.txt ». Les résultats doivent avoir le format du  corpus  annoté  « pos_reference.txt.lima »  (2  colonnes  séparées  par  une  tabulation)  et doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  pos_test.txt.pos.stanford \n",
    "-  pos_test.txt.pos.nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltkTokenize(fileIn,fileOut):\n",
    "    \n",
    "    # Ouvrir le fichier en lecture seule\n",
    "    file = open(fileIn, \"r\")\n",
    "    fileO = open(fileOut, \"w\")\n",
    "    for line in file:\n",
    "        \n",
    "        text = word_tokenize(line)\n",
    "    \n",
    "        tokens_tag = nltk.pos_tag(text)\n",
    "        \n",
    "        for pair in tokens_tag :\n",
    "            fileO.write('\\t'.join(pair))  \n",
    "            fileO.write('\\n')\n",
    "    fileO.close()\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ligne de commande à taper dans le terminal linux depuis le dossier stanford-postagger\n",
    "\n",
    "./stanford-postagger.sh models/english-left3words-distsim.tagger ../pos_test.txt > ../pos_test.txt.pos.stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_test.txt\"\n",
    "fileOutNltk = \"pos_test.txt.pos.nltk\"\n",
    "fileOutStan = \"pos_test.txt.pos.stanford\"\n",
    "\n",
    "\n",
    "nltkTokenize(fileIn,fileOutNltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Convertir les résultats des deux POS taggers en utilisant les étiquettes universelles (Annexe 1). Il faut utiliser la table de correspondance « POSTags_PTB_Universal_Linux.txt » pour réaliser cette  transformation  (PTB => Universal).  Les  résultats  de  cette  conversion  doivent  être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  pos_test.txt.pos.stanford.univ \n",
    "-  pos_test.txt.pos.nltk.univ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire PTB vers Universal\n",
    "my_dico = {}\n",
    "\n",
    "#on crée un dictionnaire à partir des tags de reference\n",
    "fileR = open(fileRef2,'r')\n",
    "for line in fileR:\n",
    "    tuples = line.split()\n",
    "    my_dico[tuples[0]] = tuples[1]\n",
    "fileR.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagPTBUnivNltk(fileIn, dico, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    \n",
    "    # modification des tags vers les tags univ\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        if tup:\n",
    "            # on écrit le groupe de mots initial\n",
    "            fileO.write(tup[0]+'\\t'+ dico[tup[1]])\n",
    "           \n",
    "            fileO.write('\\n')\n",
    "        else:\n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "setTagPTBUnivNltk(fileOutNltk, my_dico, \"pos_test.txt.pos.nltk.univ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajout d'étiquettes dans le fichier PTB vers Univ car nous avons constater un manque pour les étiquettes suivantes :\n",
    "-  $ => CUR\n",
    "-  `` => .\n",
    "-  '' => ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modification de quelques passage dans pos_test.txt qui font planter le pos tagger de stanford\n",
    "\n",
    " - ligne 154 => during the past 3 1/2 years    -> during the past 3 years\n",
    " - ligne 230 => inventivness of an { offending } country   -> inventivness of an offending country\n",
    " - ligne 255 =>  ( During its centennial year , The Wall Street Journal will report events of the past century that stand as milestones of American business history .\n",
    ")   -> During its centennial year , The Wall Street Journal will report events of the past century that stand as milestones of American business history .\n",
    " - ligne 302 => Canadian dollars ( US 13.73 ) .   ->  Canadian dollars US 13.73 . \n",
    " - ligne 452 => Sheep Chase \" ( Kodansha , 320 pages , 18.95 )  -> Sheep Chase \" Kodansha , 320 pages , 18.95\n",
    " - ligne 467 => ( every Japanese under 40 seems to be fluent in Beatles lyrics )  ->  every Japanese under 40 seems to be fluent in Beatles lyrics\n",
    " - ligne 470  => Have Wa \" ( Macmillan , 339 pages ,  17.95 )  ->   Have Wa \" Macmillan , 339 pages , 17.95\n",
    " - ligne 478  => Funny Business \" ( Soho , 228 pages ,  17.95 )   ->  Funny Business \" Soho , 228 pages , 17.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagPTBUnivStan(fileIn, dico, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    checkTag = 0\n",
    "    w = \"\"\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        for word in tup:\n",
    "            print(word)\n",
    "            for letter in word:\n",
    "                if(letter == \"_\"):\n",
    "                    fileO.write(\"\\t\")\n",
    "                    checkTag = 1\n",
    "                if(checkTag == 1 and letter != \"_\"):\n",
    "                    w = w+letter\n",
    "                if(letter != \"_\" and checkTag == 0):\n",
    "                    fileO.write(letter)\n",
    "            fileO.write(\" \"+dico[w]+\" \")\n",
    "            fileO.write('\\n')\n",
    "            checkTag = 0\n",
    "            w = \"\"\n",
    "    fileI.close()\n",
    "    fileO.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b261edbd6765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msetTagPTBUnivStan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileOutStan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_dico\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"pos_test.txt.pos.stanford.univ\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-6727b7fffbd7>\u001b[0m in \u001b[0;36msetTagPTBUnivStan\u001b[1;34m(fileIn, dico, fileOut)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mletter\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheckTag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mfileO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mletter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mfileO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdico\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mfileO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mcheckTag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "setTagPTBUnivStan(fileOutStan,my_dico,\"pos_test.txt.pos.stanford.univ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Lancer l’évaluation des deux POS taggers. Pour réaliser cette évaluation, il faut supprimer la ligne vide (séparant les phrases) dans le de fichier de référence « pos_reference.txt.univ » : \n",
    "-  python evaluate.py pos_test.txt.pos.stanford.univ pos_reference.txt.univ \n",
    "-  python evaluate.py pos_test.txt.pos.nltk.univ pos_reference.txt.univ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Quelles conclusions vous pouvez avoir à partir des résultats d’évaluation des deux POS taggers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> II. Evaluation de la reconnaissance d’entités nommées </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Utiliser le corpus annoté « ne_reference.txt.conll » pour extraire les phrases ayant servi pour produire ce corpus annoté et sauvegarder le résultat dans le fichier « ne_test.txt ». Dans ce corpus, une ligne vide indique la fin de la phrase courante. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Lancer les deux NE recognizers sur le fichier « ne_test.txt ». Les résultats doivent avoir le format  du  corpus  annoté  « ne_reference.txt.conll »  (2  colonnes  séparées  par  une tabulation) et doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  ne_test.txt.ne.stanford \n",
    "-  ne_test.txt.ne.nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Convertir  les  résultats  des  deux  NE  recognizers  en  utilisant  les  étiquettes  CoNLL-2003 (https://www.clips.uantwerpen.be/conll2003/ner/  -Annexe  2-).  Les  résultats  de  cette conversion doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  ne_test.txt.ne.stanford.conll \n",
    "-  ne_test.txt.ne.nltk.conll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Lancer  l’évaluation  des  deux  NE  recognizers.  Pour  réaliser  cette  évaluation,  il  faut supprimer  la  ligne  vide  (séparant  les  phrases)  dans  le  de  fichier  de  référence « ne_reference.txt.conll »: \n",
    "-  python evaluate.py ne_test.txt.ne.stanford.conll ne_reference.txt.conll \n",
    "-  python evaluate.py ne_test.txt.ne.nltk.conll ne_reference.txt.conll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Quelles conclusions vous pouvez avoir à partir des résultats d’évaluation des deux NE recognizers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b1dee31b2b8bd9e4c2bb4bb44ed11c551d034648e4147e6c7c5ae9371a102e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

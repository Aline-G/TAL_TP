{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Sujet 1 : Evaluation de deux plateformes open source d'analyse linguistique</u>\n",
    "\n",
    "## <u> I - Evaluation de l'analyse morpho-syntaxique </u>\n",
    "\n",
    "##### 1) Utiliser le corpus annoté « pos_reference.txt.lima » pour extraire les phrases ayant servi pour produire ce corpus annoté et sauvegarder le résultat dans le fichier « pos_test.txt  »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    for line in fileI:\n",
    "\n",
    "        tuples = line.split()\n",
    "        if tuples:\n",
    "            # On souhaite récupérer l'entièreté de la ligne à l'exception du dernier mot\n",
    "            # qui est l'annotation\n",
    "            for i in range(len(tuples)-1):\n",
    "                fileO.write(tuples[i]+\" \")\n",
    "        else:\n",
    "            # Si on arrive à une ligne vide on saute une ligne c'est un changement de phrase \n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_reference.txt.lima\"\n",
    "fileOut = \"pos_test.txt\"\n",
    "\n",
    "getText(fileIn,fileOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Convertir  les  tags  du  corpus  annoté  « pos_reference.txt.lima »  en  tags  universels  et sauvegarder le résultat dans le fichier « pos_reference.txt.univ ». Il faut utiliser les deux tables de correspondance « POSTags_LIMA_PTB_Linux.txt » et « POSTags_PTB_Universal_Linux.txt » pour réaliser cette transformation (LIMA => PTB => Universal). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagUniv(fileIn, fileRef1,fileRef2, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileR1 = open(fileRef1,'r')\n",
    "    fileR2 = open(fileRef2,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    my_dico1 = {}\n",
    "    my_dico2 = {}\n",
    "\n",
    "    #on crée deux dictionnaires à partir des tags de reference\n",
    "    for line in fileR1:\n",
    "        tuples = line.split()\n",
    "        my_dico1[tuples[0]] = tuples[1]\n",
    "    fileR1.close()\n",
    "\n",
    "    for line in fileR2:\n",
    "        tuples = line.split()\n",
    "        my_dico2[tuples[0]] = tuples[1]\n",
    "    fileR2.close()\n",
    "\n",
    "    # modification des tags vers les tags univ\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        if tup:\n",
    "            # on écrit le groupe de mots initial\n",
    "            for i in range(len(tup)-1):\n",
    "                    fileO.write(tup[i]+\" \")\n",
    "            temp = my_dico1[tup[len(tup)-1]]\n",
    "\n",
    "            fileO.write('\\t'+ my_dico2[temp])\n",
    "            fileO.write('\\n')\n",
    "        else:\n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_reference.txt.lima\"\n",
    "fileRef1 = \"POSTags_LIMA_PTB_Linux.txt\"\n",
    "fileRef2 = \"POSTags_PTB_Universal_Linux.txt\"\n",
    "fileOut = \"pos_reference.txt.univ\"\n",
    "\n",
    "\n",
    "setTagUniv(fileIn,fileRef1,fileRef2,fileOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Lancer les deux POS taggers sur le fichier « pos_test.txt ». Les résultats doivent avoir le format du  corpus  annoté  « pos_reference.txt.lima »  (2  colonnes  séparées  par  une  tabulation)  et doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  pos_test.txt.pos.stanford \n",
    "-  pos_test.txt.pos.nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltkTokenize(fileIn,fileOut):\n",
    "    \n",
    "    # Ouvrir le fichier en lecture seule\n",
    "    file = open(fileIn, \"r\")\n",
    "    fileO = open(fileOut, \"w\")\n",
    "    for line in file:\n",
    "        \n",
    "        text = word_tokenize(line)\n",
    "    \n",
    "        tokens_tag = nltk.pos_tag(text)\n",
    "        \n",
    "        for pair in tokens_tag :\n",
    "            fileO.write('\\t'.join(pair))  \n",
    "            fileO.write('\\n')\n",
    "    fileO.close()\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ligne de commande à taper dans le terminal linux depuis le dossier stanford-postagger\n",
    "\n",
    "./stanford-postagger.sh models/english-left3words-distsim.tagger ../pos_test.txt > ../pos_test.txt.pos.stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"pos_test.txt\"\n",
    "fileOutNltk = \"pos_test.txt.pos.nltk\"\n",
    "fileOutStan = \"pos_test.txt.pos.stanford\"\n",
    "\n",
    "\n",
    "nltkTokenize(fileIn,fileOutNltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Convertir les résultats des deux POS taggers en utilisant les étiquettes universelles (Annexe 1). Il faut utiliser la table de correspondance « POSTags_PTB_Universal_Linux.txt » pour réaliser cette  transformation  (PTB => Universal).  Les  résultats  de  cette  conversion  doivent  être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  pos_test.txt.pos.stanford.univ \n",
    "-  pos_test.txt.pos.nltk.univ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dictionnaire PTB vers Universal\n",
    "my_dico = {}\n",
    "\n",
    "#on crée un dictionnaire à partir des tags de reference\n",
    "fileR = open(fileRef2,'r')\n",
    "for line in fileR:\n",
    "    tuples = line.split()\n",
    "    my_dico[tuples[0]] = tuples[1]\n",
    "fileR.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagPTBUnivNltk(fileIn, dico, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    \n",
    "    # modification des tags vers les tags univ\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        if tup:\n",
    "            # on écrit le groupe de mots initial\n",
    "            fileO.write(tup[0]+'\\t'+ dico[tup[1]])\n",
    "           \n",
    "            fileO.write('\\n')\n",
    "        else:\n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "setTagPTBUnivNltk(fileOutNltk, my_dico, \"pos_test.txt.pos.nltk.univ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajout d'étiquettes dans le fichier PTB vers Univ car nous avons constater un manque pour les étiquettes suivantes :\n",
    "-  $ => CUR\n",
    "-  `` => .\n",
    "-  '' => .\n",
    "-  -LRB- => .\n",
    "-  -RRB- => ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modification d'un passage dans pos_reference.txt.lima qui fait planter le pos tagger de stanford\n",
    "\n",
    " - ligne 3293 => 3 1/2 NUM    ->  3 NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTagPTBUnivStan(fileIn, dico, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    checkTag = 0\n",
    "    w = \"\"\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        for word in tup:\n",
    "            for letter in word:\n",
    "                if(letter == \"_\"):\n",
    "                    fileO.write(\"\\t\")\n",
    "                    checkTag = 1\n",
    "                if(checkTag == 1 and letter != \"_\"):\n",
    "                    w = w+letter\n",
    "                if(letter != \"_\" and checkTag == 0):\n",
    "                    fileO.write(letter)\n",
    "            fileO.write(\" \"+dico[w]+\" \")\n",
    "            fileO.write('\\n')\n",
    "            checkTag = 0\n",
    "            w = \"\"\n",
    "    fileI.close()\n",
    "    fileO.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "setTagPTBUnivStan(fileOutStan,my_dico,\"pos_test.txt.pos.stanford.univ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Lancer l’évaluation des deux POS taggers. Pour réaliser cette évaluation, il faut supprimer la ligne vide (séparant les phrases) dans le de fichier de référence « pos_reference.txt.univ » : \n",
    "-  python evaluate.py pos_test.txt.pos.stanford.univ pos_reference.txt.univ \n",
    "-  python evaluate.py pos_test.txt.pos.nltk.univ pos_reference.txt.univ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppressEmptyLine(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    for line in fileI:\n",
    "        tup = line.split()\n",
    "        if tup:\n",
    "            fileO.write(line)\n",
    "    fileI.close()\n",
    "    fileO.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L'objectif est mainteant de se débarasser de cette erreur\n",
    "\n",
    "Warning: the reference and the candidate consists of different number of lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptData(fileIn, fileOut):\n",
    "    # compte le nombre de mot sur la ligne et recréer des lignes vides ensuite\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    \n",
    "    for line in fileI:\n",
    "        tup = line.split(\" \")\n",
    "        tupTab = line.split(\"\\t\")\n",
    "        spaces = len(tup)\n",
    "        fileO.write(line)\n",
    "        if(line.__contains__(\"'s\")):\n",
    "            fileO.write(\"remplissage \"+tupTab[len(tupTab)-1])\n",
    "        elif(line.__contains__(\"Toys ``R\\\" Us Inc.\")):\n",
    "            fileO.write(\"remplissage \"+tupTab[len(tupTab)-1])\n",
    "            fileO.write(\"remplissage \"+tupTab[len(tupTab)-1])\n",
    "        elif(line[len(tup[0])-1] == \"'\"):\n",
    "            fileO.write(\"remplissage \"+tupTab[len(tupTab)-1])\n",
    "        \n",
    "        for i in range (0, spaces-2):\n",
    "            fileO.write(\"remplissage \"+tupTab[len(tupTab)-1])\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileRefInter = \"pos_reference.txt.univ.modif\"\n",
    "fileRef = \"pos_reference.txt.univ\"\n",
    "fileRefOut = \"pos_reference.txt.univ.adjustLines\"\n",
    "\n",
    "suppressEmptyLine(fileRef, fileRefInter)\n",
    "adaptData(fileRefInter,fileRefOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ligne de commande à taper pour réaliser les evaluate\n",
    "-  python evaluateBis.py pos_test.txt.pos.stanford.univ pos_reference.txt.univ\n",
    "-  python evaluateBis.py pos_test.txt.pos.nltk.univ pos_reference.txt.univ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Quelles conclusions vous pouvez avoir à partir des résultats d’évaluation des deux POS taggers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> II. Evaluation de la reconnaissance d’entités nommées </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Utiliser le corpus annoté « ne_reference.txt.conll » pour extraire les phrases ayant servi pour produire ce corpus annoté et sauvegarder le résultat dans le fichier « ne_test.txt ». Dans ce corpus, une ligne vide indique la fin de la phrase courante. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextPartieDeux(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "\n",
    "    for line in fileI:\n",
    "\n",
    "        tuples = line.split()\n",
    "        if tuples:\n",
    "            fileO.write(tuples[0]+\" \")\n",
    "        else:\n",
    "            # Si on arrive à une ligne vide on saute une ligne c'est un changement de phrase \n",
    "            fileO.write(\"\\n\")\n",
    "\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"ne_reference.txt.conll.txt\"\n",
    "fileOut = \"ne_test.txt\"\n",
    "getTextPartieDeux(fileIn,fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Lancer les deux NE recognizers sur le fichier « ne_test.txt ». Les résultats doivent avoir le format  du  corpus  annoté  « ne_reference.txt.conll »  (2  colonnes  séparées  par  une tabulation) et doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  ne_test.txt.ne.stanford \n",
    "-  ne_test.txt.ne.nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltkTokenizeNE(fileIn,fileOut):\n",
    "    \n",
    "    file = open(fileIn, \"r\")\n",
    "    fileO = open(fileOut, \"w\")\n",
    "    for line in file:\n",
    "        \n",
    "        text = word_tokenize(line)\n",
    "    \n",
    "        tokens_tag = nltk.pos_tag(text)\n",
    "\n",
    "        ne_tree = nltk.ne_chunk(tokens_tag, binary= False)\n",
    "        \n",
    "        for tree in ne_tree.subtrees() :\n",
    "            \n",
    "            if tree.label() == \"PERSON\":\n",
    "                for branch in tree:\n",
    "                    b1 = str(branch[0])\n",
    "                    fileO.write(b1+\" \")\n",
    "                fileO.write(\"PERS\")\n",
    "                fileO.write(\"\\n\")\n",
    "            elif tree.label() == \"ORGANIZATION\" or tree.label() == \"FACILITY\":\n",
    "                for branch in tree:\n",
    "                    b1 = str(branch[0])\n",
    "                    fileO.write(b1+\" \")\n",
    "                fileO.write(\"ORG\")\n",
    "                fileO.write(\"\\n\")\n",
    "            elif tree.label() == \"LOCATION\" or tree.label() == \"GPE\" or  tree.label() == \"GSP\" :\n",
    "                for branch in tree:\n",
    "                    b1 = str(branch[0])\n",
    "                    fileO.write(b1+\" \")\n",
    "                fileO.write(\"LOC\")\n",
    "                fileO.write(\"\\n\")\n",
    "            elif tree.label() == \"MONEY\" or tree.label() == \"PERCENT\" or tree.label() == \"TIME\" or  tree.label() == \"DATE\":\n",
    "                for branch in tree:\n",
    "                    b1 = str(branch[0])\n",
    "                    fileO.write(b1+\" \")\n",
    "                fileO.write(\"MISC\")\n",
    "                fileO.write(\"\\n\")\n",
    "    fileO.close()\n",
    "    file.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"ne_test.txt\"\n",
    "fileOut = \"ne_test.txt.ne.nltk\"\n",
    "\n",
    "nltkTokenizeNE(fileIn, fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tape la commande suivante dans un terminal pour appliquer le NE recognizer de standford sur \"ne_test.txt\" :\n",
    "\n",
    "java -mx600m -cp stanford-ner.jar:lib/* edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile  ../ne_test.txt > ../ne_test.txt.ne.stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire une fonction pour que le le texte soit écrit en 2 colonnes\n",
    "def miseEnPage(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        for word in tup:\n",
    "            for letter in word:\n",
    "                if(letter == \"/\"):\n",
    "                    fileO.write(\"\\t\")\n",
    "                else : \n",
    "                    fileO.write(letter)\n",
    "            fileO.write('\\n')\n",
    "    fileI.close()\n",
    "    fileO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = \"ne_test.txt.ne.stanford\"\n",
    "fileOut = \"ne_test.txt.ne.stanford.final\"\n",
    "miseEnPage(fileIn,fileOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Convertir  les  résultats  des  deux  NE  recognizers  en  utilisant  les  étiquettes  CoNLL-2003 (https://www.clips.uantwerpen.be/conll2003/ner/  -Annexe  2-).  Les  résultats  de  cette conversion doivent être sauvegardés respectivement dans les fichiers suivants : \n",
    "-  ne_test.txt.ne.stanford.conll \n",
    "-  ne_test.txt.ne.nltk.conll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fonction qui permet de convertir les étiquettes de Stanford NE en CoNLL_2003\n",
    "# Current tag permet de récupérer l'étqiuette courante du token qui précède \n",
    "# Si les deux sont identiques on peut mettre que l'on est dans la partie I (inside) de notre entitée nommée\n",
    "# Sinon on chnage d'étiquette et donc on est au début de la nouvelle entitée nommée (Begining)\n",
    "# De la même façon que dans la référence nous ne gérons pas le fait que plusieurs entitées nommées puissent se suivre et avoir la même étiquette\n",
    "#  Exemple : Maria Bara Victor Hugo vont être étiquetés comme étant B-PERS pour Maria et I-PERS pour le reste\n",
    "#  ###\n",
    "\n",
    "def convertCoNllStan(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    currentTag =''\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        if(tup[1]!= currentTag):\n",
    "            if(tup[1] == \"PERSON\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-PERS\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"LOCATION\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-LOC\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"ORGANIZATION\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-ORG\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"MISC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-MISC\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            else:\n",
    "                fileO.write(tup[0]+\"\\t\"+tup[1]+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "        else :\n",
    "            if(tup[1] == \"PERSON\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-PERS\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"LOCATION\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-LOC\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"ORGANIZATION\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-ORG\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[1] == \"MISC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-MISC\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            else:\n",
    "                fileO.write(tup[0]+\"\\t\"+tup[1]+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "    fileI.close()\n",
    "    fileO.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileOutStan = \"ne_test.txt.ne.stanford.conll\"\n",
    "fileInStan = \"ne_test.txt.ne.stanford.final\"\n",
    "\n",
    "convertCoNllStan(fileInStan, fileOutStan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fonction qui permet de convertir les étiquettes de NLTK NE en CoNLL_2003\n",
    "# Current tag permet de récupérer l'étqiuette courante du token qui précède \n",
    "# Si les deux sont identiques on peut mettre que l'on est dans la partie I (inside) de notre entitée nommée\n",
    "# Sinon on chnage d'étiquette et donc on est au début de la nouvelle entitée nommée (Begining)\n",
    "# De la même façon que dans la référence nous ne gérons pas le fait que plusieurs entitées nommées puissent se suivre et avoir la même étiquette\n",
    "#  Exemple : Maria Bara Victor Hugo vont être étiquetés comme étant B-PERS pour Maria et I-PERS pour le reste\n",
    "#\n",
    "# La différence principale avec la fonction précédente est que l'on peut avoir des groupes de mots dans le même token qu enous avons traités comme étant B-TAG pour le premier et I-TAG pour la suite\n",
    "#  ###\n",
    "\n",
    "def convertCoNllNltk(fileIn, fileOut):\n",
    "    fileI = open(fileIn,'r')\n",
    "    fileO = open(fileOut,'w')\n",
    "    currentTag =''\n",
    "    for l in fileI:\n",
    "        tup = l.split()\n",
    "        spaces=len(tup)-1\n",
    "        if(tup[spaces]!= currentTag):\n",
    "            if(tup[spaces] == \"PERS\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-PERS\"+\"\\n\")\n",
    "                if(spaces>1):\n",
    "                   for i in range (1,spaces):\n",
    "                        fileO.write(tup[i]+\"\\t\"+\"I-PERS\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            elif(tup[spaces] == \"LOC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-LOC\"+\"\\n\")\n",
    "                if(spaces>1):\n",
    "                       for i in range (1,spaces):\n",
    "                        fileO.write(tup[i]+\"\\t\"+\"I-LOC\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            elif(tup[spaces] == \"ORG\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-ORG\"+\"\\n\")\n",
    "                if(spaces>1):\n",
    "                       for i in range (1,spaces):\n",
    "                        fileO.write(tup[i]+\"\\t\"+\"I-ORG\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            elif(tup[spaces] == \"MISC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"B-MISC\"+\"\\n\")\n",
    "                if(spaces>1):\n",
    "                       for i in range (1,spaces):\n",
    "                        fileO.write(tup[i]+\"\\t\"+\"I-MISC\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            else:\n",
    "                fileO.write(tup[0]+\"\\t\"+tup[spaces]+\"\\n\")\n",
    "                if(spaces>1):\n",
    "                       for i in range (1,spaces):\n",
    "                        fileO.write(tup[i]+\"\\t\"+\"O\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "        else :\n",
    "            if(tup[spaces] == \"PERS\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-PERS\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            elif(tup[spaces] == \"LOC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-LOC\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            elif(tup[spaces] == \"ORG\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-ORG\"+\"\\n\")\n",
    "                currentTag = tup[1]\n",
    "            elif(tup[spaces] == \"MISC\"):\n",
    "                fileO.write(tup[0]+\"\\t\"+\"I-MISC\"+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "            else:\n",
    "                fileO.write(tup[0]+\"\\t\"+tup[spaces]+\"\\n\")\n",
    "                currentTag = tup[spaces]\n",
    "    fileI.close()\n",
    "    fileO.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileInNLTK = \"ne_test.txt.ne.nltk\"\n",
    "fileOutNLTK = \"ne_test.txt.ne.nltk.conll\"\n",
    "\n",
    "convertCoNllNltk(fileInNLTK, fileOutNLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Lancer  l’évaluation  des  deux  NE  recognizers.  Pour  réaliser  cette  évaluation,  il  faut supprimer  la  ligne  vide  (séparant  les  phrases)  dans  le  de  fichier  de  référence « ne_reference.txt.conll »: \n",
    "-  python evaluateBis.py ne_test.txt.ne.stanford.conll ne_reference.txt.conll.txt.final\n",
    "-  python evaluateBis.py ne_test.txt.ne.nltk.conll ne_reference.txt.conll.txt.final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressEmptyLine(\"ne_reference.txt.conll.txt\",\"ne_reference.txt.conll.txt.final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Quelles conclusions vous pouvez avoir à partir des résultats d’évaluation des deux NE recognizers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b1dee31b2b8bd9e4c2bb4bb44ed11c551d034648e4147e6c7c5ae9371a102e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
